# rl_trader.py - Trading com Reinforcement Learning
import numpy as np
import pandas as pd
import logging
from collections import deque
import random
from typing import Dict, List, Tuple
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam


class RLTrader:
    def __init__(self, state_size: int = 10, enable_training: bool = True):
        self.state_size = state_size
        self.enable_training = enable_training
        self.logger = self._setup_logger()
        
        # Hiperpar√¢metros do RL
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95  # Discount factor
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.batch_size = 32
        
        # Modelos
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()
        
        # M√©tricas
        self.episode_rewards = []
        self.training_history = []
        
        self.logger.info("ü§ñ RL Trader inicializado com Deep Q-Learning")

    def _setup_logger(self):
        logger = logging.getLogger('RLTrader')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger

    def _build_model(self) -> Sequential:
        """Constr√≥i a rede neural Deep Q-Learning"""
        try:
            model = Sequential([
                Dense(128, input_dim=self.state_size, activation='relu'),
                Dropout(0.3),
                Dense(64, activation='relu'),
                Dropout(0.2),
                Dense(32, activation='relu'),
                Dense(3, activation='linear')  # 3 a√ß√µes: BUY=0, SELL=1, HOLD=2
            ])
            
            model.compile(
                loss='mse',
                optimizer=Adam(learning_rate=self.learning_rate)
            )
            
            return model
        except Exception as e:
            self.logger.error(f"‚ùå Erro construindo modelo: {e}")
            # Fallback para modelo simples
            model = Sequential([
                Dense(24, input_dim=self.state_size, activation='relu'),
                Dense(24, activation='relu'),
                Dense(3, activation='linear')
            ])
            model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))
            return model

    def update_target_model(self):
        """Atualiza o modelo alvo (target network)"""
        self.target_model.set_weights(self.model.get_weights())

    def _create_state(self, market_data: Dict, portfolio: Dict) -> np.ndarray:
        """Cria o estado atual para o RL"""
        try:
            # Features do mercado (normalizadas)
            rsi = market_data.get('rsi', 50)
            macd = market_data.get('macd', 0)
            macd_hist = market_data.get('macd_histogram', 0)
            bb_pos = market_data.get('bb_position', 0.5)
            volume_ratio = market_data.get('volume_ratio', 1)
            volatility = market_data.get('volatility', 0.1)
            trend_strength = market_data.get('trend_strength', 0.5)
            
            # Features do portf√≥lio
            cash_ratio = portfolio.get('cash_ratio', 0.5)
            position_size = portfolio.get('position_size', 0)
            current_pnl = portfolio.get('current_pnl', 0)
            
            # Normaliza√ß√£o
            features = [
                rsi / 100.0,                           # 0-1
                np.tanh(macd * 10),                    # Normalizado
                np.tanh(macd_hist * 100),              # Normalizado  
                bb_pos,                                # J√° normalizado
                min(volume_ratio / 3.0, 1.0),          # Limitado a 0-1
                min(volatility * 10, 1.0),             # Limitado a 0-1
                trend_strength,                        # J√° normalizado
                cash_ratio,                            # J√° normalizado
                min(position_size / 5.0, 1.0),         # Limitado a 0-1
                np.tanh(current_pnl / 100.0)           # Normalizado
            ]
            
            return np.array(features).reshape(1, -1)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro criando estado: {e}")
            return np.zeros((1, self.state_size))

    def choose_action(self, state: np.ndarray) -> int:
        """Escolhe a√ß√£o usando pol√≠tica Œµ-greedy"""
        try:
            if np.random.rand() <= self.epsilon and self.enable_training:
                # Explora√ß√£o: a√ß√£o aleat√≥ria
                return random.randrange(3)
            
            # Explora√ß√£o: melhor a√ß√£o prevista
            act_values = self.model.predict(state, verbose=0)
            return np.argmax(act_values[0])
        except Exception as e:
            self.logger.error(f"‚ùå Erro escolhendo a√ß√£o: {e}")
            return 2  # Fallback para HOLD

    def remember(self, state: np.ndarray, action: int, reward: float, 
                next_state: np.ndarray, done: bool):
        """Armazena experi√™ncia no replay memory"""
        self.memory.append((state, action, reward, next_state, done))

    def replay(self):
        """Treina o modelo com experi√™ncias anteriores"""
        if len(self.memory) < self.batch_size:
            return

        try:
            # Amostra batch aleat√≥rio
            minibatch = random.sample(self.memory, self.batch_size)
            
            states = np.array([experience[0][0] for experience in minibatch])
            next_states = np.array([experience[3][0] for experience in minibatch])
            
            # Previs√µes atuais e futuras
            current_q = self.model.predict(states, verbose=0)
            next_q = self.target_model.predict(next_states, verbose=0)
            
            # Atualiza Q-values
            for i, (state, action, reward, next_state, done) in enumerate(minibatch):
                if done:
                    target = reward
                else:
                    target = reward + self.gamma * np.amax(next_q[i])
                
                current_q[i][action] = target
            
            # Treina o modelo
            self.model.fit(states, current_q, epochs=1, verbose=0, batch_size=self.batch_size)
            
            # Decai epsilon
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro no replay: {e}")

    def calculate_reward(self, action: int, pnl: float, market_conditions: Dict, 
                        holding_period: float = 1.0) -> float:
        """Calcula recompensa baseada no resultado do trade"""
        try:
            reward = 0.0
            
            # Recompensa/Puni√ß√£o baseada no P&L (principal)
            reward += pnl * 20  # Escala o P&L
            
            # Recompensas estrat√©gicas baseadas em condi√ß√µes de mercado
            rsi = market_conditions.get('rsi', 50)
            trend = market_conditions.get('trend', 'NEUTRAL')
            volatility = market_conditions.get('volatility', 0.02)
            
            if action == 0:  # BUY
                if trend == 'UPTREND':
                    reward += 0.2
                if rsi < 35:  # RSI oversold
                    reward += 0.3
                if volatility < 0.05:  # Baixa volatilidade
                    reward += 0.1
                    
            elif action == 1:  # SELL  
                if trend == 'DOWNTREND':
                    reward += 0.2
                if rsi > 65:  # RSI overbought
                    reward += 0.3
                if volatility > 0.08:  # Alta volatilidade
                    reward += 0.1
                    
            elif action == 2:  # HOLD
                if volatility > 0.1:  # Muita volatilidade
                    reward += 0.2
                if abs(pnl) > 0.05:  # P&L significativo em posi√ß√£o aberta
                    reward += pnl * 10
            
            # Puni√ß√£o por a√ß√µes muito frequentes
            reward -= 0.05
            
            # Recompensa por holding period √≥timo
            optimal_hold = 4.0  # 4 horas
            hold_bonus = 1.0 - min(abs(holding_period - optimal_hold) / optimal_hold, 1.0)
            reward += hold_bonus * 0.1
            
            return float(reward)
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro calculando recompensa: {e}")
            return pnl * 10  # Fallback para recompensa baseada apenas no P&L

    async def get_trading_signal(self, market_data: Dict, portfolio: Dict, 
                               symbol: str) -> Dict:
        """Gera sinal de trading usando RL"""
        try:
            # Cria estado atual
            state = self._create_state(market_data, portfolio)
            
            # Escolhe a√ß√£o
            action = self.choose_action(state)
            
            # Mapeia a√ß√£o para sinal de trading
            action_map = {
                0: {'action': 'buy', 'type': 'RL_BUY', 'strength': 0.8},
                1: {'action': 'sell', 'type': 'RL_SELL', 'strength': 0.8},
                2: {'action': 'hold', 'type': 'RL_HOLD', 'strength': 0.3}
            }
            
            signal = action_map[action]
            signal['rl_state'] = state.tolist()
            signal['epsilon'] = self.epsilon
            signal['q_values'] = self.model.predict(state, verbose=0)[0].tolist()
            
            self.logger.info(f"üéØ Sinal RL {symbol}: {signal['action'].upper()} "
                           f"(Œµ={self.epsilon:.3f}, Q={signal['q_values']})")
            
            return signal
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro gerando sinal RL: {e}")
            return {'action': 'hold', 'type': 'RL_ERROR', 'strength': 0.1}

    def update_with_result(self, state: np.ndarray, action: int, pnl: float,
                         next_state: np.ndarray, market_conditions: Dict,
                         holding_period: float = 1.0, done: bool = False):
        """Atualiza o modelo com resultado do trade"""
        if not self.enable_training:
            return
            
        try:
            # Calcula recompensa
            reward = self.calculate_reward(action, pnl, market_conditions, holding_period)
            
            # Armazena experi√™ncia
            self.remember(state, action, reward, next_state, done)
            
            # Treina o modelo
            self.replay()
            
            # Atualiza target model periodicamente
            if len(self.memory) % 100 == 0:
                self.update_target_model()
                
            # Log de treinamento
            if len(self.memory) % 50 == 0:
                self.logger.info(f"üß† RL Training: Memory={len(self.memory)}, "
                               f"Œµ={self.epsilon:.3f}, Last Reward={reward:.3f}")
                               
        except Exception as e:
            self.logger.error(f"‚ùå Erro atualizando RL: {e}")

    def save_model(self, filepath: str = 'rl_trader_model.h5'):
        """Salva o modelo treinado"""
        try:
            self.model.save(filepath)
            self.logger.info(f"üíæ Modelo RL salvo: {filepath}")
        except Exception as e:
            self.logger.error(f"‚ùå Erro salvando modelo: {e}")

    def load_model(self, filepath: str = 'rl_trader_model.h5'):
        """Carrega modelo treinado"""
        try:
            self.model = tf.keras.models.load_model(filepath)
            self.epsilon = self.epsilon_min  # Minimiza explora√ß√£o ap√≥s carregar
            self.logger.info(f"üìÇ Modelo RL carregado: {filepath}")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel carregar modelo: {e}")

    def get_training_metrics(self) -> Dict:
        """Retorna m√©tricas de treinamento"""
        return {
            'epsilon': self.epsilon,
            'memory_size': len(self.memory),
            'episode_count': len(self.episode_rewards),
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'model_updated': len(self.memory) >= self.batch_size,
            'exploration_rate': self.epsilon
        }
